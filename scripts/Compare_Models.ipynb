{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import opensim as osim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from data_utils import *\n",
    "from eval_utils import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from lstm import LSTMModel\n",
    "from cnnlstm import CNNLSTMModel\n",
    "from lstmattn import LSTMAttentionModel\n",
    "from transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# define device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "\n",
    "test_data = np.load(data_dir + 'test_data.npz')\n",
    "\n",
    "grf_labels = ['GRF_x', 'GRF_y', 'GRF_z']\n",
    "muscle_labels = ['tibpost', 'tibant', 'edl', 'ehl', 'fdl', 'fhl', 'perbrev', 'perlong', 'achilles']\n",
    "\n",
    "grf_dict = {0: 'GRF_x', 1: 'GRF_y', 2: 'GRF_z'}\n",
    "muscle_dict = {0: 'tibpost', 1: 'tibant', 2: 'edl', 3: 'ehl', 4: 'fdl', 5: 'fhl', 6: 'perbrev', 7: 'perlong', 8: 'achilles'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 100, 3) (1340, 100, 9)\n"
     ]
    }
   ],
   "source": [
    "X_test = test_data['X_test']\n",
    "y_test = test_data['y_test']\n",
    "\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test data to torch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(3, 256, num_layers=2, batch_first=True, dropout=0.08941182342943683)\n",
       "  (fc): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = LSTMModel(input_size=3, \n",
    "                       hidden_size=256, \n",
    "                       num_layers=2, \n",
    "                       output_size=9, \n",
    "                       dropout=0.08941182342943683)\n",
    "\n",
    "lstm_model.load_state_dict(torch.load('../models/lstm.pth', weights_only=True))\n",
    "\n",
    "lstm_model.to(device)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(lstm_model.parameters(), \n",
    "#                        lr=0.0004070688993179255, \n",
    "#                        weight_decay=4.278679617413207e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Loss: 1532.4300537109375\n"
     ]
    }
   ],
   "source": [
    "lstm_loss, lstm_pred_tensor = eval_model(lstm_model, X_test_tensor, y_test_tensor)\n",
    "print(f\"LSTM Loss: {lstm_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tibpost: 0.0755\n",
      "tibant: 0.0343\n",
      "edl: 0.0521\n",
      "ehl: 0.0307\n",
      "fdl: 0.0676\n",
      "fhl: 0.0790\n",
      "perbrev: 0.0532\n",
      "perlong: 0.0465\n",
      "achilles: 0.0222\n"
     ]
    }
   ],
   "source": [
    "# y_test = y_test_tensor.cpu().numpy()\n",
    "lstm_pred = lstm_pred_tensor.cpu().numpy()\n",
    "\n",
    "lstm_rrmse = calc_rrmse_muscle(y_test, lstm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tibpost: 0.8238\n",
      "tibant: 0.9358\n",
      "edl: 0.8328\n",
      "ehl: 0.9285\n",
      "fdl: 0.8103\n",
      "fhl: 0.7824\n",
      "perbrev: 0.7212\n",
      "perlong: 0.6422\n",
      "achilles: 0.9910\n"
     ]
    }
   ],
   "source": [
    "lstm_r2 = calc_r2_muscle(y_test, lstm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNLSTMModel(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv1d(3, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(128, 256, num_layers=3, batch_first=True, dropout=0.17575462419723403)\n",
       "  (fc): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnlstm_model = CNNLSTMModel(input_size=3, \n",
    "                             hidden_size=256,\n",
    "                             num_layers=3, \n",
    "                             output_size=9, \n",
    "                             dropout=0.17575462419723403)\n",
    "\n",
    "cnnlstm_model.load_state_dict(torch.load('../models/cnn-lstm.pth', weights_only=True))\n",
    "\n",
    "cnnlstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-LSTM Loss: 1332.114501953125\n"
     ]
    }
   ],
   "source": [
    "cnnlstm_loss, cnnlstm_pred_tensor = eval_model(cnnlstm_model, X_test_tensor, y_test_tensor)\n",
    "print(f\"CNN-LSTM Loss: {cnnlstm_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tibpost: 0.0716\n",
      "tibant: 0.0302\n",
      "edl: 0.0404\n",
      "ehl: 0.0252\n",
      "fdl: 0.0641\n",
      "fhl: 0.0751\n",
      "perbrev: 0.0467\n",
      "perlong: 0.0409\n",
      "achilles: 0.0207\n"
     ]
    }
   ],
   "source": [
    "# y_test = y_test_tensor.cpu().numpy()\n",
    "cnnlstm_pred = cnnlstm_pred_tensor.cpu().numpy()\n",
    "\n",
    "cnnlstm_rrmse = calc_rrmse_muscle(y_test, cnnlstm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tibpost: 0.8417\n",
      "tibant: 0.9505\n",
      "edl: 0.8995\n",
      "ehl: 0.9518\n",
      "fdl: 0.8298\n",
      "fhl: 0.8032\n",
      "perbrev: 0.7851\n",
      "perlong: 0.7237\n",
      "achilles: 0.9922\n"
     ]
    }
   ],
   "source": [
    "cnnlstm_r2 =calc_r2_muscle(y_test, cnnlstm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMAttentionModel(\n",
       "  (lstm): LSTM(3, 256, num_layers=3, batch_first=True, dropout=0.08509082023364795)\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstmattn_model = LSTMAttentionModel(input_size=3, \n",
    "                                    hidden_size=256, \n",
    "                                    num_layers=3, \n",
    "                                    num_heads=8, \n",
    "                                    output_size=9, \n",
    "                                    lstm_dropout=0.08509082023364795, \n",
    "                                    attn_dropout=0.3104457129518861)\n",
    "\n",
    "lstmattn_model.load_state_dict(torch.load('../models/lstm-attn.pth', weights_only=True))\n",
    "\n",
    "lstmattn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM-Attention Loss: 979.2155151367188\n"
     ]
    }
   ],
   "source": [
    "lstmattn_loss, lstmattn_pred_tensor = eval_model(lstmattn_model, X_test_tensor, y_test_tensor)\n",
    "print(f\"LSTM-Attention Loss: {lstmattn_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tibpost: 0.0637\n",
      "tibant: 0.0268\n",
      "edl: 0.0385\n",
      "ehl: 0.0282\n",
      "fdl: 0.0657\n",
      "fhl: 0.0683\n",
      "perbrev: 0.0473\n",
      "perlong: 0.0408\n",
      "achilles: 0.0169\n"
     ]
    }
   ],
   "source": [
    "lstmattn_pred = lstmattn_pred_tensor.cpu().numpy()\n",
    "\n",
    "lstmattn_rrmse = calc_rrmse_muscle(y_test, lstmattn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tibpost: 0.8747\n",
      "tibant: 0.9608\n",
      "edl: 0.9087\n",
      "ehl: 0.9396\n",
      "fdl: 0.8208\n",
      "fhl: 0.8372\n",
      "perbrev: 0.7796\n",
      "perlong: 0.7248\n",
      "achilles: 0.9948\n"
     ]
    }
   ],
   "source": [
    "lstmattn_r2= calc_r2_muscle(y_test, lstmattn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (input_embedding): Linear(in_features=3, out_features=32, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=32, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.012122943820592716, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.012122943820592716, inplace=False)\n",
       "        (dropout2): Dropout(p=0.012122943820592716, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=32, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = TransformerModel(input_dim=3,\n",
    "                                     output_dim=9, \n",
    "                                     d_model=32, \n",
    "                                     nhead=8, \n",
    "                                     num_encoder_layers=6, \n",
    "                                     dim_feedforward=256, \n",
    "                                     dropout=0.012122943820592716)\n",
    "\n",
    "transformer_model.load_state_dict(torch.load('../models/transformer.pth', weights_only=True))\n",
    "\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Loss: 890.1572265625\n"
     ]
    }
   ],
   "source": [
    "transformer_loss, transformer_pred_tensor = eval_model(transformer_model, X_test_tensor, y_test_tensor)\n",
    "print(f\"Transformer Loss: {transformer_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tibpost: 0.0574\n",
      "tibant: 0.0251\n",
      "edl: 0.0313\n",
      "ehl: 0.0244\n",
      "fdl: 0.0534\n",
      "fhl: 0.0607\n",
      "perbrev: 0.0431\n",
      "perlong: 0.0361\n",
      "achilles: 0.0170\n"
     ]
    }
   ],
   "source": [
    "transformer_pred = transformer_pred_tensor.cpu().numpy()\n",
    "\n",
    "transformer_rrmse = calc_rrmse_muscle(y_test, transformer_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tibpost: 0.8982\n",
      "tibant: 0.9659\n",
      "edl: 0.9397\n",
      "ehl: 0.9547\n",
      "fdl: 0.8817\n",
      "fhl: 0.8717\n",
      "perbrev: 0.8169\n",
      "perlong: 0.7846\n",
      "achilles: 0.9947\n"
     ]
    }
   ],
   "source": [
    "transformer_r2 = calc_r2_muscle(y_test, transformer_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM RMSE Overall: 39.14626374012921\n",
      "CNN-LSTM RMSE Overall: 36.498143321045575\n",
      "LSTM-Attention RMSE Overall: 31.292418308225095\n",
      "Transformer RMSE Overall: 29.835502999717793\n"
     ]
    }
   ],
   "source": [
    "lstm_rmse_overall = calc_rmse_overall(y_test, lstm_pred)\n",
    "print(f\"LSTM RMSE Overall: {lstm_rmse_overall}\")\n",
    "\n",
    "cnnlstm_rmse_overall = calc_rmse_overall(y_test, cnnlstm_pred)\n",
    "print(f\"CNN-LSTM RMSE Overall: {cnnlstm_rmse_overall}\")\n",
    "\n",
    "lstmattn_rmse_overall = calc_rmse_overall(y_test, lstmattn_pred)\n",
    "print(f\"LSTM-Attention RMSE Overall: {lstmattn_rmse_overall}\")\n",
    "\n",
    "transformer_rmse_overall = calc_rmse_overall(y_test, transformer_pred)\n",
    "print(f\"Transformer RMSE Overall: {transformer_rmse_overall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM RRMSE Overall: 0.010057245780650328\n",
      "CNN-LSTM RRMSE Overall: 0.009376905043963846\n",
      "LSTM-Attention RRMSE Overall: 0.008039478405550209\n",
      "Transformer RRMSE Overall: 0.007665175625685436\n"
     ]
    }
   ],
   "source": [
    "lstm_rrmse_overall = calc_rrmse_overall(y_test, lstm_pred)\n",
    "print(f\"LSTM RRMSE Overall: {lstm_rrmse_overall}\")\n",
    "\n",
    "cnnlstm_rrmse_overall = calc_rrmse_overall(y_test, cnnlstm_pred)\n",
    "print(f\"CNN-LSTM RRMSE Overall: {cnnlstm_rrmse_overall}\")\n",
    "\n",
    "lstmattn_rrmse_overall = calc_rrmse_overall(y_test, lstmattn_pred)\n",
    "print(f\"LSTM-Attention RRMSE Overall: {lstmattn_rrmse_overall}\")\n",
    "\n",
    "transformer_rrmse_overall = calc_rrmse_overall(y_test, transformer_pred)\n",
    "print(f\"Transformer RRMSE Overall: {transformer_rrmse_overall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM RRMSE Weighted: 0.03625386730483253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-LSTM RRMSE Weighted: 0.03331635888064981\n",
      "LSTM-Attention RRMSE Weighted: 0.0294192024535717\n",
      "Transformer RRMSE Weighted: 0.027417547690643182\n"
     ]
    }
   ],
   "source": [
    "lstm_rrmse_weighted = calc_rrmse_weighted(y_test, lstm_pred)\n",
    "print(f\"LSTM RRMSE Weighted: {lstm_rrmse_weighted}\")\n",
    "\n",
    "cnnlstm_rrmse_weighted = calc_rrmse_weighted(y_test, cnnlstm_pred)\n",
    "print(f\"CNN-LSTM RRMSE Weighted: {cnnlstm_rrmse_weighted}\")\n",
    "\n",
    "lstmattn_rrmse_weighted = calc_rrmse_weighted(y_test, lstmattn_pred)\n",
    "print(f\"LSTM-Attention RRMSE Weighted: {lstmattn_rrmse_weighted}\")\n",
    "\n",
    "transformer_rrmse_weighted = calc_rrmse_weighted(y_test, transformer_pred)\n",
    "print(f\"Transformer RRMSE Weighted: {transformer_rrmse_weighted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM RMSPE: 0.4455128557411234\n",
      "CNN-LSTM RMSPE: 0.4417430053560954\n",
      "LSTM-Attention RMSPE: 0.43236024565927444\n",
      "Transformer RMSPE: 0.3589254023390982\n"
     ]
    }
   ],
   "source": [
    "lstm_rmspe = calc_rmspe_overall(y_test, lstm_pred)\n",
    "print(f\"LSTM RMSPE: {lstm_rmspe}\")\n",
    "\n",
    "cnnlstm_rmspe = calc_rmspe_overall(y_test, cnnlstm_pred)\n",
    "print(f\"CNN-LSTM RMSPE: {cnnlstm_rmspe}\")\n",
    "\n",
    "lstmattn_rmspe = calc_rmspe_overall(y_test, lstmattn_pred)\n",
    "print(f\"LSTM-Attention RMSPE: {lstmattn_rmspe}\")\n",
    "\n",
    "transformer_rmspe = calc_rmspe_overall(y_test, transformer_pred)\n",
    "print(f\"Transformer RMSPE: {transformer_rmspe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM R2: 0.9934202175245972\n",
      "CNN-LSTM R2: 0.994280310605302\n",
      "LSTM-Attention R2: 0.9957955503590777\n",
      "Transformer R2: 0.9961779388698756\n"
     ]
    }
   ],
   "source": [
    "lstm_r2 = calc_r2_overall(y_test, lstm_pred)\n",
    "print(f\"LSTM R2: {lstm_r2}\")\n",
    "\n",
    "cnnlstm_r2 = calc_r2_overall(y_test, cnnlstm_pred)\n",
    "print(f\"CNN-LSTM R2: {cnnlstm_r2}\")\n",
    "\n",
    "lstmattn_r2 = calc_r2_overall(y_test, lstmattn_pred)\n",
    "print(f\"LSTM-Attention R2: {lstmattn_r2}\")\n",
    "\n",
    "transformer_r2 = calc_r2_overall(y_test, transformer_pred)\n",
    "print(f\"Transformer R2: {transformer_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_idx = 0\n",
    "\n",
    "# true = y_test_tensor[sample_idx].cpu().numpy()\n",
    "# pred = lstm_pred_tensor[sample_idx].cpu().numpy()\n",
    "\n",
    "# perc_stance = np.linspace(0, 1, 100)\n",
    "\n",
    "# fig, axes = plt.subplots(3, 3, figsize=(15, 10))  # Create subplots for 9 muscles\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for i in range(9):\n",
    "#     axes[i].plot(perc_stance, true[:, i], label=\"True\")\n",
    "#     axes[i].plot(perc_stance, pred[:, i], label=\"Predicted\", linestyle='dashed')\n",
    "#     axes[i].set_title(muscle_dict[i])\n",
    "#     axes[i].set_xlabel(\"Percent Normalized Stance (%)\")\n",
    "#     axes[i].set_ylabel(\"Muscle Force (N)\")\n",
    "#     axes[i].legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
